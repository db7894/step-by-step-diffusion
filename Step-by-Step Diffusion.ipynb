{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb98849b-9da3-488a-8ac8-3356b7946f64",
   "metadata": {},
   "source": [
    "# Step-by-Step Diffusion\n",
    "\n",
    "Pedagogical implementations following Nakkiran et al. [Step-by-Step Diffusion: An Elementary Tutorial](https://arxiv.org/abs/2406.08929)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83616e4a-4327-4229-b556-d7e282aac011",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac65af-2d1f-441a-b109-840261219ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a29305-4e2f-43c1-8f34-a3e4025f1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device found. Using Apple Silicon GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not found. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a6d02-b812-48c1-b232-2725f10e92c4",
   "metadata": {},
   "source": [
    "# Gaussian Diffusion\n",
    "\n",
    "The Gaussian Diffusion forward process is initially defined as\n",
    "$$ x_{t+1} := x_t + η_t \\ , \\ η_t \\sim N (0, σ^2). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05248d-511a-4419-96dc-9dd1f36e1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGaussianDiffusion:\n",
    "    def __init__(self, num_timesteps, sigma_q):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.sigma_q = sigma_q\n",
    "        self.delta_t = 1.0 / num_timesteps\n",
    "\n",
    "    def forward_process(self, x0, return_trajectories=False):\n",
    "        \"\"\"\n",
    "        Implements the forward process x_{t+Δt} := x_t + η_t, η_t ~ N(0, σ_q^2 Δt)\n",
    "        from equation 11 on page 7 of the tutorial\n",
    "        \"\"\"\n",
    "        print(f\"self num timesteps {self.num_timesteps}\")\n",
    "        xt = x0\n",
    "        if return_trajectories:\n",
    "            trajectories = [x0]\n",
    "        for t in range(self.num_timesteps):\n",
    "            eta = torch.randn_like(xt) * self.sigma_q * torch.sqrt(torch.tensor(self.delta_t))\n",
    "            xt = xt + eta\n",
    "            if return_trajectories: trajectories.append(xt)\n",
    "        if return_trajectories: return torch.stack(trajectories)\n",
    "        return xt\n",
    "\n",
    "    def sample_xt(self, x0, t):\n",
    "        \"\"\"\n",
    "        Samples x_t ~ N(x_0, σ_t^2), where σ_t := σ_q√t\n",
    "        \"\"\"\n",
    "        sigma_t = self.sigma_q * torch.sqrt(torch.tensor(t * self.delta_t))\n",
    "        noise = torch.randn_like(x0) * sigma_t\n",
    "        return x0 + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e257097-bc0c-485a-ad8b-099b164e1373",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bc7f8-2970-46c6-a8f5-ccd7edaefa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def visualize_diffusion(x0, diffusion, num_frames=50):\n",
    "    trajectories = diffusion.forward_process(x0, return_trajectories=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    scatter = ax.scatter([], [], alpha=0.6)\n",
    "    title = ax.set_title(f'Timestep: 0/{diffusion.num_timesteps}')\n",
    "    \n",
    "    def update(frame):\n",
    "        i = int(frame * diffusion.num_timesteps / num_frames)\n",
    "        data = trajectories[i].tolist()\n",
    "        scatter.set_offsets(data)\n",
    "        title.set_text(f'Timestep: {i}/{diffusion.num_timesteps}')\n",
    "        return scatter, title\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=num_frames, interval=50, blit=True)\n",
    "    plt.close(fig)\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf12f6b-9d7a-4ce0-ab23-c510b5b2a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 1000\n",
    "sigma_q = 1.0\n",
    "diffusion = SimpleGaussianDiffusion(num_timesteps, sigma_q)\n",
    "\n",
    "# generate points on a circle\n",
    "theta = torch.linspace(0, 2*torch.pi, 100)\n",
    "x0 = torch.stack([torch.cos(theta), torch.sin(theta)], dim=1)\n",
    "\n",
    "anim = visualize_diffusion(x0, diffusion)\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a7e18-ebae-46af-95c4-741b3cc9f925",
   "metadata": {},
   "source": [
    "# DDPM-Like Reverse Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02edc541-308d-4714-b70a-eff7828b03f7",
   "metadata": {},
   "source": [
    "We'll start with the presentation of the Diffusion Reverse process in Section 1.3, then work towards DDPM. __Fact 1__ tells us that for a small variance $\\sigma$ and the Gaussian diffusion process from (1), the conditional distribution $p(x_{t-1} \\ | \\ x_t)$ is itself Gaussian. This means, given a fixed value for $x_t$ at some timestep, there is some mean parameter $\\mu \\in \\mathbb{R}^d$ s.t. $$ p(x_{t-1} \\ | \\ x_t) \\approx \\mathcal{N}(x_{t-1}; \\ \\mu \\ , \\ \\sigma^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae3f12-c7a5-4bf4-9566-1b08f1e474b9",
   "metadata": {},
   "source": [
    "Since we know everything about the distribution except its mean, we need only learn the mean $\\mu_{t-1}(x_t)$ to learn the full conditional distribution; we can learn this by regression. \n",
    "\n",
    "Using a fact about conditional expectation, we can turn the estimate of the conditional expectations $$ \\mu_t(z) := \\mathbb{E} \\left[ x_t \\ | \\ x_{t + \\Delta t} = z \\right] $$ into a standard regression problem. In particular, to learn the set of functions $\\mu_t$ for every timestep $t \\in {0,\\Delta t,..., 1 - \\Delta t}$ in a _training_ phase, we estimate the functions from i.i.d. samples of $x_0$ by optimizing the denoising regression objective \n",
    "$$ \\mu_t = \\underset{f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d }{\\text{argmin}} \\underset{{x_t, x_{t + \\Delta t}}}{\\mathbb{E}} || f(x_{t+ \\Delta t}) - x_t ||_2^2 $$\n",
    "with a neural network parameterizing $f$. In the inference phase, we use estimated functions in the following DDPM-like Stochastic Reverse Sampler:\n",
    "\n",
    "\n",
    "__Algorithm 1__: \\\n",
    "For input sample $x_t$ and timestep $t$, output:\n",
    "$$ \\hat{x}_{t - \\Delta t} \\leftarrow \\mu_{t - \\Delta t}(x_t) + \\mathcal{N}(0, \\sigma_q^2\\Delta t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b2a6b-b231-448f-b68a-bb98f9e3e09d",
   "metadata": {},
   "source": [
    "We'll use a basic model (UNet) for reverse sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb038d-3ca9-40fd-bce7-25897708d937",
   "metadata": {},
   "source": [
    "## Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392faf8e-8ab5-482b-8687-29cb7c73222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, num_timesteps, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sigma_q = torch.sqrt(self.beta)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f6658-19ba-434e-8275-1a88a1cc7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, out_channels, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # t is a float between 0 and 1\n",
    "        t = t.view(-1, 1, 1, 1).expand(-1, 1, x.shape[2], x.shape[3])\n",
    "        x = torch.cat([x, t], dim=1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return self.conv3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0e216-2598-496a-acdf-98fc1adde57a",
   "metadata": {},
   "source": [
    "## DDPM Train Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e874e67-4cb7-447c-aa7b-5ad447212158",
   "metadata": {},
   "source": [
    "Following Pseudocode 1 (Section 2.2) to implement DDPM train loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4385c6-1cf1-40ad-9dc9-de64b27c7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpm_train_loss(model, x0, num_timesteps, sigma_q):\n",
    "    device = next(model.parameters()).device\n",
    "    # Sample t uniformly from {0, delta_t, 2*delta_t, ..., (T-1)*delta_t}\n",
    "    t = torch.randint(0, num_timesteps, (x0.shape[0],), device=device)\n",
    "\n",
    "    # generate noise ~ N(0,1) and scale below\n",
    "    noise_t = torch.randn_like(x0, device=device)\n",
    "    noise_delta = torch.randn_like(x0, device=device)    \n",
    "\n",
    "    # xt <- x0 + N(0, σ^2_q t)\n",
    "    # we scale N(0,1)\n",
    "    # scale t by delta_t so it lies in [0,1]\n",
    "    # print(f\"x0 shape {x0.shape} t shape {t.shape} noise_t shape {noise_t.shape} sigma_q shape {sigma_q.shape}\")\n",
    "    t_float = t.float() / num_timesteps\n",
    "    sigma_t = sigma_q[t]  # Index sigma_q with t\n",
    "    xt = x0 + noise_t * sigma_t.view(-1,1,1,1) * t_float.view(-1,1,1,1) # * torch.sqrt(t.float() / num_timesteps).view(-1,1,1,1)\n",
    "    \n",
    "    # but here we're interested in x_{t + delta t} which I'll just call x\n",
    "    # the distribution we sample from here is N(0, sigma_q^2 * delta_t) where delta_t = 1 / num_timesteps\n",
    "    x_t_plus_1 = xt + noise_delta * sigma_t.view(-1,1,1,1) * torch.sqrt(torch.tensor((1 / num_timesteps), device=device)).view(-1,1,1,1)\n",
    "\n",
    "    # model prediction of x_t from x_t_plus_1\n",
    "    predicted_xt = model(x_t_plus_1, (t.float() + 1) / num_timesteps)\n",
    "\n",
    "    # L ← ||fθ(x, t + Δt) - xt||²₂\n",
    "    loss = nn.MSELoss()(predicted_xt, xt)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb518f-51ea-4a88-a630-740bea5adb41",
   "metadata": {},
   "source": [
    "## DDPM Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e89d4-2f52-424c-a560-1e313f3e4879",
   "metadata": {},
   "source": [
    "Pseudocode 2 for DDPM sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f05f6d-0aa0-4015-b9f0-ed9336375d96",
   "metadata": {},
   "source": [
    "__Input__: Trained model $f_{\\theta}$ \\\n",
    "__Data__: Terminal variance $\\sigma_q$; step-size $\\Delta t$. \\\n",
    "__Output__: $x_0$\n",
    "\n",
    "$x_1 \\leftarrow \\mathcal{N}(0,\\sigma_q^2)$ \\\n",
    "__for__ $t = 1, (1 - \\Delta t), (t - 2\\Delta t), ..., \\Delta t$ __do__\n",
    "\n",
    "$\\quad\\quad \\eta \\leftarrow \\mathcal{N}(0,\\sigma_q^2\\Delta t)$\n",
    "\n",
    "$\\quad\\quad x_{t - \\Delta t} \\leftarrow f_{\\theta}(x_t,t) + \\eta$\n",
    "\n",
    "__end__ \\\n",
    "__return__ $x_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8319c-0129-4189-8d67-d736121b8307",
   "metadata": {},
   "source": [
    "Algorithm 1 for the DDPM-like sampler and Claim 1 (page 9) say that to sample from $x_{t - \\Delta t}$, it suffices to first sample from $x_t$ then sample from a Gaussian distribution centered around $\\mathbb{E}[x_{t - \\Delta t} \\ | \\ x_t]$. This is what DDPM does in Algorithm 1 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb307b2-c845-4335-a23d-016dbbb93253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpm_sampling(model, shape, num_timesteps, sigma_q):\n",
    "    # print(f\"batch shape {shape} sigma_q shape {sigma_q.shape} \\n sigma_q {sigma_q}\")\n",
    "    # sigma_t = \n",
    "    x = torch.randn(shape) * sigma_q[-1] # sample x1 from N(0,1)\n",
    "    delta_t = 1.0 / num_timesteps\n",
    "    for t in torch.arange(1, 0, -delta_t):\n",
    "        t_index = int(t * num_timesteps) - 1\n",
    "        \n",
    "        # https://pytorch.org/docs/stable/generated/torch.full.html\n",
    "        t_tensor = torch.full((shape[0],), t, dtype=torch.float32) / num_timesteps\n",
    "\n",
    "        # eta samples from N(0, sigma)q^2 * delta_t\n",
    "        eta = torch.randn(shape) * sigma_q[t_index] * torch.sqrt(torch.tensor(delta_t))\n",
    "\n",
    "        # x_t_minus_delta_t <- model(xt,t) + eta\n",
    "        # we're basically going to update x that we had started with.\n",
    "        x_update = model(x, t_tensor) + eta\n",
    "        x = x_update\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f853e-dad3-4085-b770-7bd4e844f8ea",
   "metadata": {},
   "source": [
    "# DDIM Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719c332-ea9f-4bc6-aad7-7c12bb7a4377",
   "metadata": {},
   "source": [
    "This is Section 2.2, Pseudocode 3 / Algorithm 2. \n",
    "\n",
    "__Input__: Trained model $f_{\\theta}$ \\\n",
    "__Data__: Terminal variance $\\sigma_q$; step-size $\\Delta t$. \\\n",
    "__Output__: $x_0$ \\\n",
    "$x_1 \\leftarrow \\mathcal{N}(0,\\sigma_q^2)$ \\\n",
    "__for__ $t = 1,(1 - \\Delta t), (1 - 2\\Delta t),..., \\Delta t,0$ __do__ \\\n",
    "$\\quad\\quad \\lambda \\leftarrow \\frac{\\sqrt{t}}{\\sqrt{t - \\Delta t} + \\sqrt{t}}$ \\\n",
    "$\\quad\\quad x_{t - \\Delta t} \\leftarrow x_t + \\lambda(f_{\\theta}(x_t,t) - x_t)$ \\\n",
    "__end__ \\\n",
    "__return__ $x_0$\n",
    "\n",
    "Note that the original formulation of Algorithm 2 (page 16 of tutorial) uses $\\lambda := \\left( \\dfrac{\\sigma_t}{\\sigma_{t - \\Delta t} + \\sigma_t} \\right)$. It's valid to just use $t$ in the formulation:\n",
    "\n",
    "Since we define \n",
    "\\begin{align}\n",
    "\\lambda := \\left( \\dfrac{\\sigma_t}{\\sigma_{t - \\Delta t} + \\sigma_t} \\right)\n",
    "&= \\left( \\dfrac{\\sigma_q\\sqrt{t}}{\\sigma_q\\sqrt{t-\\Delta t} + \\sigma_q\\sqrt{t}} \\right) \\\\\n",
    "&= \\left( \\dfrac{\\sqrt{t}}{\\sqrt{t-\\Delta t} + \\sqrt{t}} \\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422f63d-e51a-4d55-9765-3337da48dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIM(nn.Module):\n",
    "    def __init__(self, num_timesteps, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        device = next(model.parameters()).device\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sigma_q = torch.sqrt(self.beta)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb1324-25f2-4136-b136-6d52c0cb240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddim_sampling(model, shape, num_timesteps, sigma_q):\n",
    "    device = next(model.parameters()).device\n",
    "    x = torch.randn(shape, device=device) * sigma_q[-1] # sample x1 from N(0,1)\n",
    "    delta_t = 1.0 / num_timesteps\n",
    "    for t in torch.arange(1, 0, -delta_t, device=device):\n",
    "        t_index = int(t * num_timesteps) - 1\n",
    "        \n",
    "        t_tensor = t.float().expand(shape[0]).view(-1, 1, 1, 1)\n",
    "\n",
    "        lambda_denominator = torch.sqrt(t - delta_t) + torch.sqrt(t) + 1e-8\n",
    "        lambd = torch.div(torch.sqrt(t_tensor), lambda_denominator)\n",
    "\n",
    "        pred_x = model(x, t_tensor)\n",
    "\n",
    "        # x_t_minus_delta_t <- lambda * (model(xt,t) - x_t) + x_t\n",
    "        # Compute x_{t-Δt} = λ * (f_θ(x_t, t) - x_t) + x_t \n",
    "        # here x_{t-Δt} is just the x we started with that we'll update each loop iter\n",
    "        x_update = lambd * (pred_x - x) + x\n",
    "        x = x_update\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c6c37-8989-497c-9909-ec0f04de8b26",
   "metadata": {},
   "source": [
    "# Flow Matching (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d22971-c016-4888-8b8c-d91fe48649c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
